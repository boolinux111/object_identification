{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7dF5NKaN5Ti"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# output_scenes 폴더를 /content/content 로 복사\n",
        "!cp -r /content/drive/MyDrive/capstone_code/output_scenes /content\n",
        "\n",
        "# 복사된 내용 확인\n",
        "!ls /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Hx5T0l0aOhD"
      },
      "outputs": [],
      "source": [
        "!pip install face_recognition\n",
        "!pip install deep_sort_realtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZKWqff_YtiR"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/serengil/deepface.git\n",
        "%cd deepface\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 씬 분할 순서 정렬\n",
        "- 씬 분할 이후 파일을 불러올 때 순서를 정렬하기 위한 코드"
      ],
      "metadata": {
        "id": "UZLSjQtXQZa-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WYfWDrojOixf"
      },
      "outputs": [],
      "source": [
        "import glob, re\n",
        "\n",
        "mp4_paths = sorted(\n",
        "    glob.glob(\"/content/output_scenes/Scene_*.mp4\"),\n",
        "    key=lambda fname: [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', fname)]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 주인공에 대한 얼굴 임베딩을 사전에 생성하는 코드\n",
        "- 현재 실행코드에선 주인공에 얼굴 임베딩을 사전 생성하지 않고, 처음 본 객체에 새롭게 ID를 부여하도록 구현함.\n",
        "- 현재는 필요 없지만, 해당 코드를 먼저 실행하지 않으면 'KerasHistory' object has no attribute 'layer' 오류가 발생함.\n",
        "- 해당 문제가 발생하는 이유를 찾아 수정할 수 있다면 하는 것이 좋아보임.\n",
        "- 주인공에 집중에서 해설을 하고 싶다면, 해당 코드에 임베딩을 원하는 파일에 주인공과 관련된 이미지 파일을 넣으면 됨.\n"
      ],
      "metadata": {
        "id": "Ir1rQRLYQjmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETXPt5DrP4Bf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from deepface import DeepFace\n",
        "\n",
        "def build_actor_embeddings(actor_db_path, model_name=\"Facenet512\", detector_backend=\"mtcnn\"):\n",
        "    \"\"\"\n",
        "    actor_db_path 디렉토리 내 하위 폴더(배우 이름)에서\n",
        "    각 이미지 임베딩을 평균내어 {actor_name: avg_embedding} 형태의 dict 반환\n",
        "    \"\"\"\n",
        "    actor_embeddings = {}\n",
        "    for actor in os.listdir(actor_db_path):\n",
        "        actor_folder = os.path.join(actor_db_path, actor)\n",
        "        if not os.path.isdir(actor_folder) or actor.startswith('.'):\n",
        "            continue\n",
        "\n",
        "        embs = []\n",
        "        for img in os.listdir(actor_folder):\n",
        "            if not img.lower().endswith(('.jpg','.jpeg','.png')):\n",
        "                continue\n",
        "            img_path = os.path.join(actor_folder, img)\n",
        "            rep = DeepFace.represent(\n",
        "                img_path,\n",
        "                model_name=model_name,\n",
        "                detector_backend=detector_backend,\n",
        "                enforce_detection=False\n",
        "            )\n",
        "            if rep and isinstance(rep, list) and 'embedding' in rep[0]:\n",
        "                embs.append(np.array(rep[0]['embedding']))\n",
        "\n",
        "        if embs:\n",
        "            actor_embeddings[actor] = np.mean(embs, axis=0)\n",
        "            print(f\"  ▶ {actor}: {len(embs)}장 이미지로 임베딩 생성\")\n",
        "        else:\n",
        "            print(f\"  ⚠️ {actor}: 유효한 얼굴 이미지 없음\")\n",
        "\n",
        "    return actor_embeddings\n",
        "\n",
        "# 예시: 한 번만 실행해서 저장해 두기\n",
        "if __name__ == \"__main__\":\n",
        "    db_path = \"/content/drive/MyDrive/image\"\n",
        "    actor_embeddings = build_actor_embeddings(db_path)\n",
        "    # 나중에 불러 쓰기 좋게 .npy로 저장\n",
        "    np.save(\"/content/actor_embeddings.npy\", actor_embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmNjsR_KKzvg"
      },
      "source": [
        "#최종 구현단계 (앞에서 구현한 코드 + 앞,뒤 프레임까지 비교)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXmDsJxHG5my"
      },
      "source": [
        "## YOLO 객체 검출 후\n",
        "- YOLO의 객체 검출을 이용해 frames의 파일을 생성\n",
        "- 해당 객체의 바운딩 박스를 JSON 파일의 형태로 저장"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchreid"
      ],
      "metadata": {
        "id": "2hS57jrJ0ojp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 기존 알고리즘에 얼굴 정렬 알고리즘 추가함(모든 프레임 분석)\n",
        "- 기존 알고리즘보다 얼굴을 인식하는 정확도는 상대적으로 오른 것을 확인할 수 있었음.\n",
        "- 성능 확인을 위해 모든 프레임을 분석\n",
        "- 너무 세세한 분석에 영상적인 의미를 잃어버림."
      ],
      "metadata": {
        "id": "QAyH6kLkoCg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchreid\n",
        "from mtcnn import MTCNN\n",
        "from deepface import DeepFace\n",
        "from tqdm import tqdm\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "\n",
        "# --- 설정 ---\n",
        "FRAMES_DIR       = '/content/drive/MyDrive/capstone_code/frames_and_detections_allframes/frames'\n",
        "DETECTIONS_JSON  = '/content/drive/MyDrive/capstone_code/frames_and_detections_allframes/content/detections.json'\n",
        "OUTPUT_DIR       = '/content/output_pipeline_DeepSORT'\n",
        "OUTPUT_FACE_DIR  = '/content/output_image'\n",
        "FACE_THRESH      = 0.6\n",
        "BODY_THRESH      = 0.7\n",
        "DEVICE           = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# --- 디렉터리 생성 ---\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_FACE_DIR, exist_ok=True)\n",
        "\n",
        "# --- 모델 및 트래커 초기화 ---\n",
        "face_detector    = MTCNN()\n",
        "face_model_name  = 'ArcFace'\n",
        "body_model       = torchreid.models.build_model(\n",
        "    name='resnet50_ibn_a', num_classes=1000, loss='softmax', pretrained=True\n",
        ")\n",
        "body_model.to(DEVICE).eval()\n",
        "tracker = DeepSort(max_age=3, n_init=3, max_iou_distance=0.3)\n",
        "\n",
        "# --- 칼만필터 dt 보정 ---\n",
        "internal_tracker = tracker.tracker\n",
        "kf = internal_tracker.kf\n",
        "frame_interval = 10\n",
        "for i in range(4):\n",
        "    kf._motion_mat[i, i+4] = frame_interval\n",
        "\n",
        "# --- 갤러리 및 ID 관리 ---\n",
        "person_gallery = {}\n",
        "next_person_id = 1\n",
        "final_id       = {}\n",
        "pid_to_canonical_tid = {}\n",
        "\n",
        "# --- 얼굴 정렬 템플릿 및 함수 ---\n",
        "TEMPLATE_5PTS = np.array([\n",
        "    [38.2946, 51.6963], [73.5318, 51.5014], [56.0252, 71.7366],\n",
        "    [41.5493, 92.3655], [70.7299, 92.2041]\n",
        "], dtype=np.float32)\n",
        "\n",
        "def align_face(img, kpts, output_size=(112,112)):\n",
        "    M, _ = cv2.estimateAffinePartial2D(kpts, TEMPLATE_5PTS, method=cv2.LMEDS)\n",
        "    return cv2.warpAffine(img, M, output_size, borderValue=0)\n",
        "\n",
        "# --- 임베딩 헬퍼 ---\n",
        "def extract_face_emb(img):\n",
        "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    rep = DeepFace.represent(rgb, model_name=face_model_name, enforce_detection=False)\n",
        "    if not rep:\n",
        "        return None\n",
        "    emb = np.array(rep[0]['embedding'], dtype=np.float32)\n",
        "    return emb / np.linalg.norm(emb)\n",
        "\n",
        "def extract_body_emb(img):\n",
        "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    t = torch.from_numpy(rgb).permute(2,0,1).unsqueeze(0).float().to(DEVICE)/255.0\n",
        "    with torch.no_grad():\n",
        "        feat = body_model(t)\n",
        "    emb = feat.squeeze(0).cpu().numpy()\n",
        "    return emb / np.linalg.norm(emb)\n",
        "\n",
        "def cosine(a, b):\n",
        "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
        "\n",
        "def assign_person_id(emb, mode):\n",
        "    global next_person_id\n",
        "    pid = f\"person_{next_person_id}\"\n",
        "    person_gallery[pid] = {'face': None, 'body': None}\n",
        "    person_gallery[pid][mode] = emb\n",
        "    next_person_id += 1\n",
        "    return pid\n",
        "\n",
        "# --- 매칭 헬퍼 ---\n",
        "def match_face(emb, alpha=0.6):\n",
        "    best_id, best_sim = 'unknown', 0.0\n",
        "    for pid, embs in person_gallery.items():\n",
        "        ref = embs.get('face')\n",
        "        if ref is None: continue\n",
        "        sim = cosine(emb, ref)\n",
        "        if sim > best_sim:\n",
        "            best_id, best_sim = pid, sim\n",
        "    if best_sim >= FACE_THRESH:\n",
        "        old = person_gallery[best_id]['face']\n",
        "        updated = (1-alpha)*old + alpha*emb\n",
        "        person_gallery[best_id]['face'] = updated / np.linalg.norm(updated)\n",
        "        return best_id, best_sim\n",
        "    return assign_person_id(emb, 'face'), 0.0\n",
        "\n",
        "def match_body(emb, alpha=0.7):\n",
        "    best_id, best_sim = 'unknown', 0.0\n",
        "    for pid, embs in person_gallery.items():\n",
        "        ref = embs.get('body')\n",
        "        if ref is None: continue\n",
        "        sim = cosine(emb, ref)\n",
        "        if sim > best_sim:\n",
        "            best_id, best_sim = pid, sim\n",
        "    if best_sim >= BODY_THRESH:\n",
        "        old = person_gallery[best_id]['body']\n",
        "        updated = (1-alpha)*old + alpha*emb\n",
        "        person_gallery[best_id]['body'] = updated / np.linalg.norm(updated)\n",
        "        return best_id, best_sim\n",
        "    return assign_person_id(emb, 'body'), 0.0\n",
        "\n",
        "# --- 검출 결과 로드 ---\n",
        "with open(DETECTIONS_JSON, 'r') as f:\n",
        "    dets = json.load(f)\n",
        "\n",
        "# --- 메인 파이프라인 ---\n",
        "print('Starting pipeline with dt=', frame_interval)\n",
        "for idx, fname in enumerate(tqdm(sorted(os.listdir(FRAMES_DIR)), desc='Pipeline')):\n",
        "    if not fname.lower().endswith('.jpg'): continue\n",
        "    frame = cv2.imread(os.path.join(FRAMES_DIR, fname))\n",
        "    if frame is None: continue\n",
        "\n",
        "    print(f\"\\n--- Frame {idx+1}: {fname} ---\")\n",
        "    boxes = dets.get(fname, [])\n",
        "    dets_list = [([x1, y1, x2-x1, y2-y1], conf, 'person') for x1,y1,x2,y2,conf in boxes]\n",
        "    print(f\"Detections: {len(dets_list)}\")\n",
        "\n",
        "    tracks = tracker.update_tracks(dets_list, frame=frame)\n",
        "    print(f\"Updated tracks: {[t.track_id for t in tracks]}\")\n",
        "\n",
        "    curr_ids = {t.track_id for t in tracks}\n",
        "    for old in list(final_id):\n",
        "        if old not in curr_ids:\n",
        "            print(f\"Removing track_{old} from final_id mapping\")\n",
        "            final_id.pop(old)\n",
        "\n",
        "    for t in tracks:\n",
        "        raw_tid = t.track_id\n",
        "        l, t_top, r, b = t.to_ltrb()\n",
        "        x1, y1, x2, y2 = map(int, (l, t_top, r, b))\n",
        "\n",
        "        # 기본 바운딩 박스와 트랙 ID (파란)\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "        cv2.putText(frame, f\"track_{raw_tid}\", (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
        "\n",
        "        roi = frame[y1:y2, x1:x2]\n",
        "\n",
        "        if raw_tid in final_id:\n",
        "            pid = final_id[raw_tid]\n",
        "            print(f\"track_{raw_tid} already assigned -> {pid}\")\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, pid, (x1, y1 - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "            continue\n",
        "\n",
        "        faces = face_detector.detect_faces(roi)\n",
        "        print(f\"  track_{raw_tid}, faces={len(faces)}\")\n",
        "        if faces:\n",
        "            x_f, y_f, w_f, h_f = faces[0]['box']\n",
        "            kpts_roi = np.array([faces[0]['keypoints'][k] for k in ['left_eye','right_eye','nose','mouth_left','mouth_right']], dtype=np.float32)\n",
        "            raw_face = roi[y_f:y_f+h_f, x_f:x_f+w_f]\n",
        "            kpts_raw = kpts_roi - np.array([x_f, y_f], dtype=np.float32)\n",
        "            aligned = align_face(raw_face, kpts_raw)\n",
        "\n",
        "            cv2.imwrite(os.path.join(OUTPUT_FACE_DIR, f\"raw_face_{raw_tid}_{fname}\"), raw_face)\n",
        "            cv2.imwrite(os.path.join(OUTPUT_FACE_DIR, f\"aligned_{raw_tid}_{fname}\"), aligned)\n",
        "\n",
        "            emb_f = extract_face_emb(aligned)\n",
        "            if emb_f is not None:\n",
        "                pid, sim = match_face(emb_f)\n",
        "                print(f\"Assigned FACE: track_{raw_tid} -> {pid}, sim={sim:.4f}\")\n",
        "                if pid not in pid_to_canonical_tid:\n",
        "                    pid_to_canonical_tid[pid] = raw_tid\n",
        "                final_id[raw_tid] = pid\n",
        "                continue\n",
        "\n",
        "        emb_b = extract_body_emb(roi)\n",
        "        pid, sim = match_body(emb_b)\n",
        "        print(f\"Assigned BODY: track_{raw_tid} -> {pid}, sim={sim:.4f}\")\n",
        "        if pid not in pid_to_canonical_tid:\n",
        "            pid_to_canonical_tid[pid] = raw_tid\n",
        "        final_id[raw_tid] = pid\n",
        "\n",
        "    out_path = os.path.join(OUTPUT_DIR, fname)\n",
        "    cv2.imwrite(out_path, frame)\n",
        "    print(f\"Saved {out_path}\")\n",
        "\n",
        "print('Pipeline completed.')"
      ],
      "metadata": {
        "id": "yUWP8DaJKn3i",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 기존 알고리즘에 얼굴 정렬 알고리즘 추가(5프레임 단위 분석)\n",
        "- 결과 중 가장 높은 성능을 보여줌.\n",
        "- 가장 자연스러우면서, 인물에 대한 어느 정도의 정확도도 보이고 있음.\n",
        "- 현재 ReID모델로 resnet50_ibn_a 모델을 사용하고 있는데, 재식별에 대한 성능은 그렇게 좋아보이진 않음.\n",
        "- 하지만 얼굴이 없는 객체에 대해 이전 프레임에서 분석을 했다면, 이에 대해선 재식별 성능이 어느 정도 나오는 것을 확인할 수 있었음."
      ],
      "metadata": {
        "id": "zzloCOodHOoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchreid\n",
        "from mtcnn import MTCNN\n",
        "from deepface import DeepFace\n",
        "from tqdm import tqdm\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "\n",
        "# --- 설정 ---\n",
        "FRAMES_DIR       = '/content/drive/MyDrive/capstone_code/frames_and_detections_5frames/frames'\n",
        "DETECTIONS_JSON  = '/content/drive/MyDrive/capstone_code/frames_and_detections_5frames/content/detections.json'\n",
        "OUTPUT_DIR       = '/content/output_pipeline_DeepSORT'\n",
        "OUTPUT_FACE_DIR  = '/content/output_image'\n",
        "FACE_THRESH      = 0.6\n",
        "BODY_THRESH      = 0.7\n",
        "DEVICE           = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# --- 디렉터리 생성 ---\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_FACE_DIR, exist_ok=True)\n",
        "\n",
        "# --- 모델 및 트래커 초기화 ---\n",
        "face_detector    = MTCNN()\n",
        "face_model_name  = 'ArcFace'\n",
        "body_model       = torchreid.models.build_model(\n",
        "    name='resnet50_ibn_a', num_classes=1000, loss='softmax', pretrained=True\n",
        ")\n",
        "body_model.to(DEVICE).eval()\n",
        "tracker = DeepSort(max_age=3, n_init=3, max_iou_distance=0.3)\n",
        "\n",
        "# --- 칼만필터 dt 보정 ---\n",
        "internal_tracker = tracker.tracker\n",
        "kf = internal_tracker.kf\n",
        "frame_interval = 10\n",
        "for i in range(4):\n",
        "    kf._motion_mat[i, i+4] = frame_interval\n",
        "\n",
        "# --- 갤러리 및 ID 관리 ---\n",
        "person_gallery = {}\n",
        "next_person_id = 1\n",
        "final_id       = {}\n",
        "pid_to_canonical_tid = {}\n",
        "\n",
        "# --- 얼굴 정렬 템플릿 및 함수 ---\n",
        "TEMPLATE_5PTS = np.array([\n",
        "    [38.2946, 51.6963], [73.5318, 51.5014], [56.0252, 71.7366],\n",
        "    [41.5493, 92.3655], [70.7299, 92.2041]\n",
        "], dtype=np.float32)\n",
        "\n",
        "def align_face(img, kpts, output_size=(112,112)):\n",
        "    M, _ = cv2.estimateAffinePartial2D(kpts, TEMPLATE_5PTS, method=cv2.LMEDS)\n",
        "    return cv2.warpAffine(img, M, output_size, borderValue=0)\n",
        "\n",
        "# --- 임베딩 헬퍼 ---\n",
        "def extract_face_emb(img):\n",
        "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    rep = DeepFace.represent(rgb, model_name=face_model_name, enforce_detection=False)\n",
        "    if not rep:\n",
        "        return None\n",
        "    emb = np.array(rep[0]['embedding'], dtype=np.float32)\n",
        "    return emb / np.linalg.norm(emb)\n",
        "\n",
        "def extract_body_emb(img):\n",
        "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    t = torch.from_numpy(rgb).permute(2,0,1).unsqueeze(0).float().to(DEVICE)/255.0\n",
        "    with torch.no_grad():\n",
        "        feat = body_model(t)\n",
        "    emb = feat.squeeze(0).cpu().numpy()\n",
        "    return emb / np.linalg.norm(emb)\n",
        "\n",
        "def cosine(a, b):\n",
        "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
        "\n",
        "def assign_person_id(emb, mode):\n",
        "    global next_person_id\n",
        "    pid = f\"person_{next_person_id}\"\n",
        "    person_gallery[pid] = {'face': None, 'body': None}\n",
        "    person_gallery[pid][mode] = emb\n",
        "    next_person_id += 1\n",
        "    return pid\n",
        "\n",
        "# --- 매칭 헬퍼 ---\n",
        "def match_face(emb, alpha=0.6):\n",
        "    best_id, best_sim = 'unknown', 0.0\n",
        "    for pid, embs in person_gallery.items():\n",
        "        ref = embs.get('face')\n",
        "        if ref is None: continue\n",
        "        sim = cosine(emb, ref)\n",
        "        if sim > best_sim:\n",
        "            best_id, best_sim = pid, sim\n",
        "    if best_sim >= FACE_THRESH:\n",
        "        old = person_gallery[best_id]['face']\n",
        "        updated = (1-alpha)*old + alpha*emb\n",
        "        person_gallery[best_id]['face'] = updated / np.linalg.norm(updated)\n",
        "        return best_id, best_sim\n",
        "    return assign_person_id(emb, 'face'), 0.0\n",
        "\n",
        "def match_body(emb, alpha=0.7):\n",
        "    best_id, best_sim = 'unknown', 0.0\n",
        "    for pid, embs in person_gallery.items():\n",
        "        ref = embs.get('body')\n",
        "        if ref is None: continue\n",
        "        sim = cosine(emb, ref)\n",
        "        if sim > best_sim:\n",
        "            best_id, best_sim = pid, sim\n",
        "    if best_sim >= BODY_THRESH:\n",
        "        old = person_gallery[best_id]['body']\n",
        "        updated = (1-alpha)*old + alpha*emb\n",
        "        person_gallery[best_id]['body'] = updated / np.linalg.norm(updated)\n",
        "        return best_id, best_sim\n",
        "    return assign_person_id(emb, 'body'), 0.0\n",
        "\n",
        "# --- 검출 결과 로드 ---\n",
        "with open(DETECTIONS_JSON, 'r') as f:\n",
        "    dets = json.load(f)\n",
        "\n",
        "# --- 메인 파이프라인 ---\n",
        "print('Starting pipeline with dt=', frame_interval)\n",
        "for idx, fname in enumerate(tqdm(sorted(os.listdir(FRAMES_DIR)), desc='Pipeline')):\n",
        "    if not fname.lower().endswith('.jpg'): continue\n",
        "    frame = cv2.imread(os.path.join(FRAMES_DIR, fname))\n",
        "    if frame is None: continue\n",
        "\n",
        "    print(f\"\\n--- Frame {idx+1}: {fname} ---\")\n",
        "    boxes = dets.get(fname, [])\n",
        "    dets_list = [([x1, y1, x2-x1, y2-y1], conf, 'person') for x1,y1,x2,y2,conf in boxes]\n",
        "    print(f\"Detections: {len(dets_list)}\")\n",
        "\n",
        "    tracks = tracker.update_tracks(dets_list, frame=frame)\n",
        "    print(f\"Updated tracks: {[t.track_id for t in tracks]}\")\n",
        "\n",
        "    curr_ids = {t.track_id for t in tracks}\n",
        "    for old in list(final_id):\n",
        "        if old not in curr_ids:\n",
        "            print(f\"Removing track_{old} from final_id mapping\")\n",
        "            final_id.pop(old)\n",
        "\n",
        "    for t in tracks:\n",
        "        raw_tid = t.track_id\n",
        "        l, t_top, r, b = t.to_ltrb()\n",
        "        x1, y1, x2, y2 = map(int, (l, t_top, r, b))\n",
        "\n",
        "        # 기본 바운딩 박스와 트랙 ID (파란)\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "        cv2.putText(frame, f\"track_{raw_tid}\", (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
        "\n",
        "        roi = frame[y1:y2, x1:x2]\n",
        "\n",
        "        if raw_tid in final_id:\n",
        "            pid = final_id[raw_tid]\n",
        "            print(f\"track_{raw_tid} already assigned -> {pid}\")\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, pid, (x1, y1 - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "            continue\n",
        "\n",
        "        faces = face_detector.detect_faces(roi)\n",
        "        print(f\"  track_{raw_tid}, faces={len(faces)}\")\n",
        "        if faces:\n",
        "            x_f, y_f, w_f, h_f = faces[0]['box']\n",
        "            kpts_roi = np.array([faces[0]['keypoints'][k] for k in ['left_eye','right_eye','nose','mouth_left','mouth_right']], dtype=np.float32)\n",
        "            raw_face = roi[y_f:y_f+h_f, x_f:x_f+w_f]\n",
        "            kpts_raw = kpts_roi - np.array([x_f, y_f], dtype=np.float32)\n",
        "            aligned = align_face(raw_face, kpts_raw)\n",
        "\n",
        "            cv2.imwrite(os.path.join(OUTPUT_FACE_DIR, f\"raw_face_{raw_tid}_{fname}\"), raw_face)\n",
        "            cv2.imwrite(os.path.join(OUTPUT_FACE_DIR, f\"aligned_{raw_tid}_{fname}\"), aligned)\n",
        "\n",
        "            emb_f = extract_face_emb(aligned)\n",
        "            if emb_f is not None:\n",
        "                pid, sim = match_face(emb_f)\n",
        "                print(f\"Assigned FACE: track_{raw_tid} -> {pid}, sim={sim:.4f}\")\n",
        "                if pid not in pid_to_canonical_tid:\n",
        "                    pid_to_canonical_tid[pid] = raw_tid\n",
        "                final_id[raw_tid] = pid\n",
        "                continue\n",
        "\n",
        "        emb_b = extract_body_emb(roi)\n",
        "        pid, sim = match_body(emb_b)\n",
        "        print(f\"Assigned BODY: track_{raw_tid} -> {pid}, sim={sim:.4f}\")\n",
        "        if pid not in pid_to_canonical_tid:\n",
        "            pid_to_canonical_tid[pid] = raw_tid\n",
        "        final_id[raw_tid] = pid\n",
        "\n",
        "    out_path = os.path.join(OUTPUT_DIR, fname)\n",
        "    cv2.imwrite(out_path, frame)\n",
        "    print(f\"Saved {out_path}\")\n",
        "\n",
        "print('Pipeline completed.')"
      ],
      "metadata": {
        "id": "Fz-VLSGFHQXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 기존 알고리즘에 얼굴 정렬 알고리즘 추가(10프레임 단위 분석)\n",
        "- 빠르게 성능 확인을 위해 10프레임 단위로 사람 객체를 검출한 버전으로 진행."
      ],
      "metadata": {
        "id": "WiebsFHm7lt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchreid\n",
        "from mtcnn import MTCNN\n",
        "from deepface import DeepFace\n",
        "from tqdm import tqdm\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "\n",
        "# --- 설정 ---\n",
        "FRAMES_DIR       = \"/content/drive/MyDrive/capstone_code/frames_and_detections/frames\"\n",
        "DETECTIONS_JSON  = \"/content/drive/MyDrive/capstone_code/frames_and_detections/content/detections.json\"\n",
        "OUTPUT_DIR       = '/content/output_pipeline_DeepSORT'\n",
        "OUTPUT_FACE_DIR  = '/content/output_image'\n",
        "FACE_THRESH      = 0.6\n",
        "BODY_THRESH      = 0.7\n",
        "DEVICE           = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# --- 디렉터리 생성 ---\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_FACE_DIR, exist_ok=True)\n",
        "\n",
        "# --- 모델 및 트래커 초기화 ---\n",
        "face_detector    = MTCNN()\n",
        "face_model_name  = 'ArcFace'\n",
        "body_model       = torchreid.models.build_model(\n",
        "    name='resnet50_ibn_a', num_classes=1000, loss='softmax', pretrained=True\n",
        ")\n",
        "body_model.to(DEVICE).eval()\n",
        "tracker = DeepSort(max_age=3, n_init=3, max_iou_distance=0.3)\n",
        "\n",
        "# --- 칼만필터 dt 보정 ---\n",
        "internal_tracker = tracker.tracker\n",
        "kf = internal_tracker.kf\n",
        "frame_interval = 10\n",
        "for i in range(4):\n",
        "    kf._motion_mat[i, i+4] = frame_interval\n",
        "\n",
        "# --- 갤러리 및 ID 관리 ---\n",
        "person_gallery = {}\n",
        "next_person_id = 1\n",
        "final_id       = {}\n",
        "pid_to_canonical_tid = {}\n",
        "\n",
        "# --- 얼굴 정렬 템플릿 및 함수 ---\n",
        "TEMPLATE_5PTS = np.array([\n",
        "    [38.2946, 51.6963], [73.5318, 51.5014], [56.0252, 71.7366],\n",
        "    [41.5493, 92.3655], [70.7299, 92.2041]\n",
        "], dtype=np.float32)\n",
        "\n",
        "def align_face(img, kpts, output_size=(112,112)):\n",
        "    M, _ = cv2.estimateAffinePartial2D(kpts, TEMPLATE_5PTS, method=cv2.LMEDS)\n",
        "    return cv2.warpAffine(img, M, output_size, borderValue=0)\n",
        "\n",
        "# --- 임베딩 헬퍼 ---\n",
        "def extract_face_emb(img):\n",
        "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    rep = DeepFace.represent(rgb, model_name=face_model_name, enforce_detection=False)\n",
        "    if not rep:\n",
        "        return None\n",
        "    emb = np.array(rep[0]['embedding'], dtype=np.float32)\n",
        "    return emb / np.linalg.norm(emb)\n",
        "\n",
        "def extract_body_emb(img):\n",
        "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    t = torch.from_numpy(rgb).permute(2,0,1).unsqueeze(0).float().to(DEVICE)/255.0\n",
        "    with torch.no_grad():\n",
        "        feat = body_model(t)\n",
        "    emb = feat.squeeze(0).cpu().numpy()\n",
        "    return emb / np.linalg.norm(emb)\n",
        "\n",
        "def cosine(a, b):\n",
        "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
        "\n",
        "def assign_person_id(emb, mode):\n",
        "    global next_person_id\n",
        "    pid = f\"person_{next_person_id}\"\n",
        "    person_gallery[pid] = {'face': None, 'body': None}\n",
        "    person_gallery[pid][mode] = emb\n",
        "    next_person_id += 1\n",
        "    return pid\n",
        "\n",
        "# --- 매칭 헬퍼 ---\n",
        "def match_face(emb, alpha=0.6):\n",
        "    best_id, best_sim = 'unknown', 0.0\n",
        "    for pid, embs in person_gallery.items():\n",
        "        ref = embs.get('face')\n",
        "        if ref is None: continue\n",
        "        sim = cosine(emb, ref)\n",
        "        if sim > best_sim:\n",
        "            best_id, best_sim = pid, sim\n",
        "    if best_sim >= FACE_THRESH:\n",
        "        old = person_gallery[best_id]['face']\n",
        "        updated = (1-alpha)*old + alpha*emb\n",
        "        person_gallery[best_id]['face'] = updated / np.linalg.norm(updated)\n",
        "        return best_id, best_sim\n",
        "    return assign_person_id(emb, 'face'), 0.0\n",
        "\n",
        "def match_body(emb, alpha=0.7):\n",
        "    best_id, best_sim = 'unknown', 0.0\n",
        "    for pid, embs in person_gallery.items():\n",
        "        ref = embs.get('body')\n",
        "        if ref is None: continue\n",
        "        sim = cosine(emb, ref)\n",
        "        if sim > best_sim:\n",
        "            best_id, best_sim = pid, sim\n",
        "    if best_sim >= BODY_THRESH:\n",
        "        old = person_gallery[best_id]['body']\n",
        "        updated = (1-alpha)*old + alpha*emb\n",
        "        person_gallery[best_id]['body'] = updated / np.linalg.norm(updated)\n",
        "        return best_id, best_sim\n",
        "    return assign_person_id(emb, 'body'), 0.0\n",
        "\n",
        "# --- 검출 결과 로드 ---\n",
        "with open(DETECTIONS_JSON, 'r') as f:\n",
        "    dets = json.load(f)\n",
        "\n",
        "# --- 메인 파이프라인 ---\n",
        "print('Starting pipeline with dt=', frame_interval)\n",
        "for idx, fname in enumerate(tqdm(sorted(os.listdir(FRAMES_DIR)), desc='Pipeline')):\n",
        "    if not fname.lower().endswith('.jpg'): continue\n",
        "    frame = cv2.imread(os.path.join(FRAMES_DIR, fname))\n",
        "    if frame is None: continue\n",
        "\n",
        "    print(f\"\\n--- Frame {idx+1}: {fname} ---\")\n",
        "    boxes = dets.get(fname, [])\n",
        "    dets_list = [([x1, y1, x2-x1, y2-y1], conf, 'person') for x1,y1,x2,y2,conf in boxes]\n",
        "    print(f\"Detections: {len(dets_list)}\")\n",
        "\n",
        "    tracks = tracker.update_tracks(dets_list, frame=frame)\n",
        "    print(f\"Updated tracks: {[t.track_id for t in tracks]}\")\n",
        "\n",
        "    curr_ids = {t.track_id for t in tracks}\n",
        "    for old in list(final_id):\n",
        "        if old not in curr_ids:\n",
        "            print(f\"Removing track_{old} from final_id mapping\")\n",
        "            final_id.pop(old)\n",
        "\n",
        "    for t in tracks:\n",
        "        raw_tid = t.track_id\n",
        "        l, t_top, r, b = t.to_ltrb()\n",
        "        x1, y1, x2, y2 = map(int, (l, t_top, r, b))\n",
        "\n",
        "        # 기본 바운딩 박스와 트랙 ID (파란)\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "        cv2.putText(frame, f\"track_{raw_tid}\", (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
        "\n",
        "        roi = frame[y1:y2, x1:x2]\n",
        "\n",
        "        if raw_tid in final_id:\n",
        "            pid = final_id[raw_tid]\n",
        "            print(f\"track_{raw_tid} already assigned -> {pid}\")\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, pid, (x1, y1 - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "            continue\n",
        "\n",
        "        faces = face_detector.detect_faces(roi)\n",
        "        print(f\"  track_{raw_tid}, faces={len(faces)}\")\n",
        "        if faces:\n",
        "            x_f, y_f, w_f, h_f = faces[0]['box']\n",
        "            kpts_roi = np.array([faces[0]['keypoints'][k] for k in ['left_eye','right_eye','nose','mouth_left','mouth_right']], dtype=np.float32)\n",
        "            raw_face = roi[y_f:y_f+h_f, x_f:x_f+w_f]\n",
        "            kpts_raw = kpts_roi - np.array([x_f, y_f], dtype=np.float32)\n",
        "            aligned = align_face(raw_face, kpts_raw)\n",
        "\n",
        "            cv2.imwrite(os.path.join(OUTPUT_FACE_DIR, f\"raw_face_{raw_tid}_{fname}\"), raw_face)\n",
        "            cv2.imwrite(os.path.join(OUTPUT_FACE_DIR, f\"aligned_{raw_tid}_{fname}\"), aligned)\n",
        "\n",
        "            emb_f = extract_face_emb(aligned)\n",
        "            if emb_f is not None:\n",
        "                pid, sim = match_face(emb_f)\n",
        "                print(f\"Assigned FACE: track_{raw_tid} -> {pid}, sim={sim:.4f}\")\n",
        "                if pid not in pid_to_canonical_tid:\n",
        "                    pid_to_canonical_tid[pid] = raw_tid\n",
        "                final_id[raw_tid] = pid\n",
        "                continue\n",
        "\n",
        "        emb_b = extract_body_emb(roi)\n",
        "        pid, sim = match_body(emb_b)\n",
        "        print(f\"Assigned BODY: track_{raw_tid} -> {pid}, sim={sim:.4f}\")\n",
        "        if pid not in pid_to_canonical_tid:\n",
        "            pid_to_canonical_tid[pid] = raw_tid\n",
        "        final_id[raw_tid] = pid\n",
        "\n",
        "    out_path = os.path.join(OUTPUT_DIR, fname)\n",
        "    cv2.imwrite(out_path, frame)\n",
        "    print(f\"Saved {out_path}\")\n",
        "\n",
        "print('Pipeline completed.')"
      ],
      "metadata": {
        "id": "RZ6mwJAv7Xbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 성능을 높일 수 있는 방법\n",
        "- 임베딩 관리 전략의 수정, 얼굴 인식의 정확도를 높이기 위해선 객체가 정면을 보고 있어야 함.\n",
        "- MTCNN의 경우에는 검출한 얼굴에 대한 보정을 진행하는 기능은 없음.\n",
        "- 트래킹 하이퍼파라미터의 조정\n",
        "- 갤러리의 구조 개선(갤러리에서 유사도가 높은 다른 ID와 병합하는 방식을 통해 구조 개선)"
      ],
      "metadata": {
        "id": "iKHdg2gdkn4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReID 모델에 사용할 수 있는 모델\n",
        "| 모델 이름             | 특징                                                 |\n",
        "| ----------------- | -------------------------------------------------- |\n",
        "| `osnet_ain_x1_0`  | IBN-A(InPlace−ABN) 기반 변형으로, 색상·조명 변화에 더 강건         |\n",
        "| `osnet_x1_0`      | 오리지널 OSNet (IBN 없이), IBN 버전보다 살짝 성능 차이가 있을 수 있음    |\n",
        "| `resnet50_ibn_a`  | ResNet-50 + IBN-A, 대용량 백본으로 강력한 표현력 제공             |\n",
        "| `resnet101_ibn_a` | ResNet-101 + IBN-A, 더 깊은 네트워크로 추가 성능 향상 가능         |\n",
        "| `mgn`             | Multi-Granularity Network, global+part 기반 복합 특징 추출 |\n",
        "| `pcb`             | Part-based Convolutional Baseline, 파트별 세분화된 임베딩    |\n"
      ],
      "metadata": {
        "id": "4DgmVtHa5OXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 추가 수정본"
      ],
      "metadata": {
        "id": "cjy4bD1poOBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchreid\n",
        "from mtcnn import MTCNN\n",
        "from deepface import DeepFace\n",
        "from tqdm import tqdm\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "\n",
        "# --- 설정 ---\n",
        "FRAMES_DIR       = '/content/drive/MyDrive/capstone_code/frames_and_detections/frames'\n",
        "DETECTIONS_JSON  = '/content/drive/MyDrive/capstone_code/frames_and_detections/content/detections.json'\n",
        "OUTPUT_DIR       = '/content/output_pipeline_DeepSORT'\n",
        "OUTPUT_FACE_DIR  = '/content/output_image'\n",
        "FACE_THRESH      = 0.6\n",
        "BODY_THRESH      = 0.7\n",
        "DEVICE           = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# --- 디렉터리 생성 ---\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_FACE_DIR, exist_ok=True)\n",
        "\n",
        "# --- 모델 및 트래커 초기화 ---\n",
        "face_detector    = MTCNN()\n",
        "face_model_name  = 'ArcFace'\n",
        "body_model       = torchreid.models.build_model(\n",
        "    name='resnet50_ibn_a', num_classes=1000, loss='softmax', pretrained=True\n",
        ")\n",
        "body_model.to(DEVICE).eval()\n",
        "tracker = DeepSort(max_age=3, n_init=3, max_iou_distance=0.3)\n",
        "\n",
        "# --- 칼만필터 dt 보정 ---\n",
        "internal_tracker = tracker.tracker\n",
        "kf = internal_tracker.kf\n",
        "frame_interval = 10\n",
        "for i in range(4):\n",
        "    kf._motion_mat[i, i+4] = frame_interval\n",
        "\n",
        "# --- 갤러리 및 ID 관리 ---\n",
        "person_gallery = {}\n",
        "next_person_id = 1\n",
        "final_id       = {}\n",
        "pid_to_canonical_tid = {}\n",
        "\n",
        "# --- 얼굴 정렬 템플릿 및 함수 ---\n",
        "TEMPLATE_5PTS = np.array([\n",
        "    [38.2946, 51.6963], [73.5318, 51.5014], [56.0252, 71.7366],\n",
        "    [41.5493, 92.3655], [70.7299, 92.2041]\n",
        "], dtype=np.float32)\n",
        "\n",
        "def align_face(img, kpts, output_size=(112,112)):\n",
        "    M, _ = cv2.estimateAffinePartial2D(kpts, TEMPLATE_5PTS, method=cv2.LMEDS)\n",
        "    return cv2.warpAffine(img, M, output_size, borderValue=0)\n",
        "\n",
        "# --- 임베딩 헬퍼 ---\n",
        "def extract_face_emb(img):\n",
        "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    rep = DeepFace.represent(rgb, model_name=face_model_name, enforce_detection=False)\n",
        "    if not rep:\n",
        "        return None\n",
        "    emb = np.array(rep[0]['embedding'], dtype=np.float32)\n",
        "    return emb / np.linalg.norm(emb)\n",
        "\n",
        "def extract_body_emb(img):\n",
        "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    t = torch.from_numpy(rgb).permute(2,0,1).unsqueeze(0).float().to(DEVICE)/255.0\n",
        "    with torch.no_grad():\n",
        "        feat = body_model(t)\n",
        "    emb = feat.squeeze(0).cpu().numpy()\n",
        "    return emb / np.linalg.norm(emb)\n",
        "\n",
        "def cosine(a, b):\n",
        "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
        "\n",
        "def assign_person_id(emb, mode):\n",
        "    global next_person_id\n",
        "    pid = f\"person_{next_person_id}\"\n",
        "    person_gallery[pid] = {'face': None, 'body': None}\n",
        "    person_gallery[pid][mode] = emb\n",
        "    next_person_id += 1\n",
        "    return pid\n",
        "\n",
        "# --- 매칭 헬퍼 ---\n",
        "def match_face(emb, alpha=0.6):\n",
        "    best_id, best_sim = 'unknown', 0.0\n",
        "    for pid, embs in person_gallery.items():\n",
        "        ref = embs.get('face')\n",
        "        if ref is None: continue\n",
        "        sim = cosine(emb, ref)\n",
        "        if sim > best_sim:\n",
        "            best_id, best_sim = pid, sim\n",
        "    if best_sim >= FACE_THRESH:\n",
        "        old = person_gallery[best_id]['face']\n",
        "        updated = (1-alpha)*old + alpha*emb\n",
        "        person_gallery[best_id]['face'] = updated / np.linalg.norm(updated)\n",
        "        return best_id, best_sim\n",
        "    return assign_person_id(emb, 'face'), 0.0\n",
        "\n",
        "def match_body(emb, alpha=0.7):\n",
        "    best_id, best_sim = 'unknown', 0.0\n",
        "    for pid, embs in person_gallery.items():\n",
        "        ref = embs.get('body')\n",
        "        if ref is None: continue\n",
        "        sim = cosine(emb, ref)\n",
        "        if sim > best_sim:\n",
        "            best_id, best_sim = pid, sim\n",
        "    if best_sim >= BODY_THRESH:\n",
        "        old = person_gallery[best_id]['body']\n",
        "        updated = (1-alpha)*old + alpha*emb\n",
        "        person_gallery[best_id]['body'] = updated / np.linalg.norm(updated)\n",
        "        return best_id, best_sim\n",
        "    return assign_person_id(emb, 'body'), 0.0\n",
        "\n",
        "# --- 검출 결과 로드 ---\n",
        "with open(DETECTIONS_JSON, 'r') as f:\n",
        "    dets = json.load(f)\n",
        "\n",
        "# --- 메인 파이프라인 ---\n",
        "print('Starting pipeline with dt=', frame_interval)\n",
        "for idx, fname in enumerate(tqdm(sorted(os.listdir(FRAMES_DIR)), desc='Pipeline')):\n",
        "    if not fname.lower().endswith('.jpg'): continue\n",
        "    frame = cv2.imread(os.path.join(FRAMES_DIR, fname))\n",
        "    if frame is None: continue\n",
        "\n",
        "    print(f\"\\n--- Frame {idx+1}: {fname} ---\")\n",
        "    boxes = dets.get(fname, [])\n",
        "    dets_list = [([x1, y1, x2-x1, y2-y1], conf, 'person') for x1,y1,x2,y2,conf in boxes]\n",
        "    print(f\"Detections: {len(dets_list)}\")\n",
        "\n",
        "    tracks = tracker.update_tracks(dets_list, frame=frame)\n",
        "    print(f\"Updated tracks: {[t.track_id for t in tracks]}\")\n",
        "\n",
        "    curr_ids = {t.track_id for t in tracks}\n",
        "    for old in list(final_id):\n",
        "        if old not in curr_ids:\n",
        "            print(f\"Removing track_{old} from final_id mapping\")\n",
        "            final_id.pop(old)\n",
        "\n",
        "    for t in tracks:\n",
        "        raw_tid = t.track_id\n",
        "        l, t_top, r, b = t.to_ltrb()\n",
        "        x1, y1, x2, y2 = map(int, (l, t_top, r, b))\n",
        "\n",
        "        # 기본 바운딩 박스와 트랙 ID (파란)\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "        cv2.putText(frame, f\"track_{raw_tid}\", (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
        "\n",
        "        roi = frame[y1:y2, x1:x2]\n",
        "\n",
        "        if raw_tid in final_id:\n",
        "            pid = final_id[raw_tid]\n",
        "            print(f\"track_{raw_tid} already assigned -> {pid}\")\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, pid, (x1, y1 - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "            continue\n",
        "\n",
        "        faces = face_detector.detect_faces(roi)\n",
        "        print(f\"  track_{raw_tid}, faces={len(faces)}\")\n",
        "        if faces:\n",
        "            x_f, y_f, w_f, h_f = faces[0]['box']\n",
        "            kpts_roi = np.array([faces[0]['keypoints'][k] for k in ['left_eye','right_eye','nose','mouth_left','mouth_right']], dtype=np.float32)\n",
        "            raw_face = roi[y_f:y_f+h_f, x_f:x_f+w_f]\n",
        "            kpts_raw = kpts_roi - np.array([x_f, y_f], dtype=np.float32)\n",
        "            aligned = align_face(raw_face, kpts_raw)\n",
        "\n",
        "            cv2.imwrite(os.path.join(OUTPUT_FACE_DIR, f\"raw_face_{raw_tid}_{fname}\"), raw_face)\n",
        "            cv2.imwrite(os.path.join(OUTPUT_FACE_DIR, f\"aligned_{raw_tid}_{fname}\"), aligned)\n",
        "\n",
        "            emb_f = extract_face_emb(aligned)\n",
        "            if emb_f is not None:\n",
        "                pid, sim = match_face(emb_f)\n",
        "                print(f\"Assigned FACE: track_{raw_tid} -> {pid}, sim={sim:.4f}\")\n",
        "                if pid not in pid_to_canonical_tid:\n",
        "                    pid_to_canonical_tid[pid] = raw_tid\n",
        "                final_id[raw_tid] = pid\n",
        "                continue\n",
        "\n",
        "        emb_b = extract_body_emb(roi)\n",
        "        pid, sim = match_body(emb_b)\n",
        "        print(f\"Assigned BODY: track_{raw_tid} -> {pid}, sim={sim:.4f}\")\n",
        "        if pid not in pid_to_canonical_tid:\n",
        "            pid_to_canonical_tid[pid] = raw_tid\n",
        "        final_id[raw_tid] = pid\n",
        "\n",
        "    out_path = os.path.join(OUTPUT_DIR, fname)\n",
        "    cv2.imwrite(out_path, frame)\n",
        "    print(f\"Saved {out_path}\")\n",
        "\n",
        "print('Pipeline completed.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbL2TmamoQUH",
        "outputId": "c9dfcd52-d39f-4b67-a911-b91951eca0e1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting pipeline with dt= 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pipeline: 100%|██████████| 303/303 [05:04<00:00,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 최종 결과를 영상으로 제작"
      ],
      "metadata": {
        "id": "j6KSvvWsnNx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "FRAMES_DIR = '/content/output_pipeline_DeepSORT'  # Re-ID 후 이미지들이 저장된 폴더\n",
        "VIDEO_PATH = '/content/result_DeepSORT.mp4'   # 최종 비디오 경로\n",
        "FPS = 10                             # 원본 영상과 동일하게 설정하세요\n",
        "\n",
        "# 프레임 파일명 정렬\n",
        "frame_files = sorted([f for f in os.listdir(FRAMES_DIR) if f.endswith('.jpg')])\n",
        "if not frame_files:\n",
        "    raise RuntimeError(\"재식별된 프레임 이미지가 없습니다.\")\n",
        "\n",
        "# 첫 프레임으로부터 프레임 크기(해상도) 가져오기\n",
        "first = cv2.imread(os.path.join(FRAMES_DIR, frame_files[0]))\n",
        "h, w = first.shape[:2]\n",
        "\n",
        "# VideoWriter 초기화 (코덱: MP4용 H.264)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "writer = cv2.VideoWriter(VIDEO_PATH, fourcc, FPS, (w, h))\n",
        "\n",
        "# 모든 프레임을 차례로 쓰기\n",
        "for fn in frame_files:\n",
        "    img = cv2.imread(os.path.join(FRAMES_DIR, fn))\n",
        "    writer.write(img)\n",
        "\n",
        "writer.release()\n",
        "print(f\"✅ 비디오 저장 완료: {VIDEO_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOLaFlfsgPXK",
        "outputId": "70b547f8-813d-4867-d70f-77b176010976"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 비디오 저장 완료: /content/result_DeepSORT.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SrRWmkjvnC9l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}